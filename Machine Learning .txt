Machine Learning


Machine learning is a field within AI - the science of making machine inteligent 

Machine learning ofcses on automated knowledge acquisition
	- mainly through the design and implementation of algoriths
	- these algorithms require empirical data as imput 
	
Machine laerning alorithms learn from data and often produce a predictive model as their output 
	models can then be use to make predictions as new data arrives
	
	
different algorithm require different input data
	
Three machine learning techniques are (three C's):
	
	- collaborative filtering (recommendations)
	
		helps users find items of relevance
			
			amoung a potentially vast nuber of choices
			based on comparision of preferences between users
			preferences can be either explicit (stated) or implicit(observed)
		
		- domain agnostsic 
		- relationships between users and items and not items themselves
		- amazon uses collaborative filtering to find users with similar taste
		
		algorithms:
			
			alternating least square algorith for product recommendations
			mllib only support movie rating, can't do product recommendations yet 
			
			supports both implicit and explicit feedback from input data
				
				implicit from observing user behavour 
				explicit from rating provided by the users 
			
			
	
	- clustering
	
			- clustering is unsupervised learning 
				- beings with no structure defined
				- analysis data and defined a structure 
			
			- used to discover structures in collectsion of data werer the structure was not previously defined
			- discover which clusters (grouping) naturally occur in data by examiining various properties of the input data
			- clustering is often used for exploratory analysis
				- divide huge amount of data into smaller groups
				- can then tune analysis for each group 
				
			algorithms:
			
				- k-means
				- streaming k-meansgaussian mixutre model
				- power interation clustering (PIC)
				- latent dirichlet allocation (LDA) 
			
				
			examples:
			
				marketing segmentations 
					- group similar custers in order to target them effectively
					
				finding related new articles
					- google news 
				
				epidemiological studies 
					- indentifing root causes
					
				computer vision (groups of pixels that cohere into objects)
					- related pixel clustered to recognize faces or liscense plates
	
	- classification
	
		- supervised learning 
			- require training with data that has known labels
			- a classifier can then label new data based on what it learned in training
		- start with known answer (label) e.g. a species 
		- use a dataset to train the model 
		
		examples:
		
			spam filtering - train the model based on what you say is or isnt spam
			oncology - train the model using images of tumors 
			risk analysis - train using financial records of customers who do/don't default
				- system will eventually learn to identify risks 
				
		algorithsm:

			- logistic regression
			- linear support vector machines (svms)
			- decision trees
			- random forests
			- gradient boosted decision tress
			- naive bayes
			- least-squared regression
			- ridge regression
			- lasso regression 
			
		classification corresponds to a predciting a categorical label
			- can have multiple labels (what you're trying to predict)
			- simplest case in binary classication (y/n answers)			-
				-> this is the most frequent use
					- 'will this person pay be able to get a loan?'
					- 'is this tumor malignat?'
		
		regression corresponds to predicting a contineous numberical value 
			- value to be predicted can be bounded or unbounded 
				- unbounded is literall get a numberr
				- bounded is mean the number corresponds to a range which corresponds to a catagory 
			- simplest case is linear regression
			- often used to predict prices or values
				- 'how much should my home sell for?'
				- 'how many peope will vote in this election'
		
			
				
	overfitting -> fitting noice instead of signal 	
	
			adds extra term B (beta) which change the model 
				
			regularisaton is impllemented in the algoirth thta produces the model 
			
				two common types of regulatiseion for linear regression:		
					- L1 or LASSO regulation
						- tends to encourage some B to be zero 
						
					- L2 or redige (tikhonov) regularisaton
					
					
				
				
	algorithms
	
		many algorithms - no best algorithm
		better to have more data - more accurate results 
		some algorithms are better for low amount of data some are better at high amounts
		low data algorithms are better are worse at high data volumes and vice versa 
		
		
MLlib

	sparks machine learning library
	provides many algorithms that can be implemented by sparks
	
	- supports all of the base data types in spark
	- includes data types useful in linear algebra and machine learning
	- must be floating point numbers 
	
	
	suppports basic statistis functionality:
	
		- summary statistics
		- correlations
		- sampling
		- hypothesis testing 
		- random data generation
		- kernal density estimation 
		
	statistcs functionality is contained in the mllib.stat module 
	
	feature extractor in mllib refers to transforming data in order to prepare it as input for a machine learning algorithm
	
	suports sparse and dense vectors and matrixes
		
		dense -> (0,0,0,0,1,3,0,0) raw vector, include zeros -> this is a problem as it is a waste of memory
		sparse -> (8, [5, 6], [1, 3]) maps out the zeros in the dense vector ('length',['indexes containing zeros']['value of zeros'])
		
		
	elementwiseproduct 
	
		multiplies each element by a user supplied weight 
		
	standard scalar
	
		operations on an rdd 
		
		can do mean or standard deviation on rdd 
		
	
	dimensionality reduction
	
		refers to reducing the numbers of features to a subset that contain the most infomation 
		
			-> simplify
			-> reduce noise 
			

Spark ML 

	before machine learning algorithms can be applied the data usually needs to be transformed many time to get it to the right format
		
		-> once this is done the model can be trained and then used to obtain predictions
		
		- this is sequence of event is a pipeline
		- e.g. convert data types, scale, fill in missing data (mean), apply algorithm, traing model, get predictions 
		
		a pipeline consists of: 
		
			Transformation: transform one dataset to another dataset
			
			Estimator: algorith that can be fit onto one dataset to produce a model 
			
			
	Spark ML prodives a high levle api for machine learning pipelines 
	
		main abstractions:
		
			Dataframe - Spark ML data types
			
				- RDD's of row objects 
				
					- RDD's have a name and a type set of columns 
				
				- dataframes contain metadata describing the columns 
			
			Transformation - transform one dataset to another dataset -> one dataframe to another 
			
			Estimator - algorith that can be fit onto one dataset to produce a model 
			
				- run on RDD to produce a model 
				
					- a model is a transformer,  it transforms one dataframe to another 
			
			Pipeline - chain multiple steps to produce a ML workflow
			
			
			
		
		
			
	
	
	
